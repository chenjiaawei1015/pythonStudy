# python笔记 管道,Manager,进程池

## 管道(了解)

- 对于多生产者多消费者, 没有做到数据的安全. 如果需要做到数据的安全, 需要另外进行加锁处理
- multiprocessing 模块的 Queue = 管道 + 锁

### 参数和方法介绍

- 创建管道的类

  Pipe([duplex]):在进程之间创建一条管道，并返回元组（conn1,conn2）,其中conn1，conn2表示管道两端的连接对象，强调一点：必须在产生Process对象之前产生管道

- 参数介绍

  dumplex:默认管道是全双工的，如果将duplex射成False，conn1只能用于接收，conn2只能用于发送

- 主要方法

  - conn1.recv():接收conn2.send(obj)发送的对象。如果没有消息可接收，recv方法会一直阻塞。如果连接的另外一端已经关闭，那么recv方法会抛出EOFError
  - conn1.send(obj):通过连接发送对象。obj是与序列化兼容的任意对象

- 其他方法

  - conn1.close():关闭连接。如果conn1被垃圾回收，将自动调用此方法
  - conn1.fileno():返回连接使用的整数文件描述符
  - conn1.poll([timeout]):如果连接上的数据可用，返回True。timeout指定等待的最长时限。如果省略此参数，方法将立即返回结果。如果将timeout射成None，操作将无限期地等待数据到达
  - conn1.recv_bytes([maxlength]):接收c.send_bytes()方法发送的一条完整的字节消息。maxlength指定要接收的最大字节数。如果进入的消息，超过了这个最大值，将引发IOError异常，并且在连接上无法进行进一步读取。如果连接的另外一端已经关闭，再也不存在任何数据，将引发EOFError异常
  - conn.send_bytes(buffer [, offset [, size]])：通过连接发送字节数据缓冲区，buffer是支持缓冲区接口的任意对象，offset是缓冲区中的字节偏移量，而size是要发送字节数。结果数据以单条消息的形式发出，然后调用c.recv_bytes()函数进行接收 
  - conn1.recv_bytes_into(buffer [, offset]):接收一条完整的字节消息，并把它保存在buffer对象中，该对象支持可写入的缓冲区接口（即bytearray对象或类似的对象）。offset指定缓冲区中放置消息处的字节位移。返回值是收到的字节数。如果消息长度大于可用的缓冲区空间，将引发BufferTooShort异常

### 初步使用

```python
from multiprocessing import Pipe
from multiprocessing import Process


def func(pipe_right):
    data = pipe_right.recv()
    print(f'子进程收到数据 {data}')

    pipe_right.send('123')
    pipe_right.close()


if __name__ == '__main__':
    pipe_left, pipe_right = Pipe()

    process = Process(target=func, args=(pipe_right,))
    process.start()

    pipe_right.close()

    pipe_left.send('abc')
    data = pipe_left.recv()
    print(f'主进程接收数据 {data}')
```

### 注意点

特别注意管道端点的正确管理问题。**如果是生产者或消费者中都没有使用管道的某个端点，就应将它关闭**。这也说明了为何在**生产者中关闭了管道的输出端，在消费者中关闭管道的输入端**。如果忘记执行这些步骤，程序可能在消费者中的recv（）操作上挂起。管道是由操作系统进行引用计数的，必须在所有进程中关闭管道后才能生成EOFError异常。因此，在生产者中关闭管道不会有任何效果，除非消费者也关闭了相同的管道端点

### EOFError

```python
from multiprocessing import Process, Pipe


def f(parent_conn, child_conn):
    parent_conn.close()  # 不写close将不会引发EOFError
    while True:
        try:
            print(child_conn.recv())
        except EOFError:
            child_conn.close()


if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(parent_conn, child_conn,))
    p.start()
    child_conn.close()
    parent_conn.send('hello')
    parent_conn.close()
    p.join()
```

### 利用管道实现进程间通信

```python
from multiprocessing import Process, Pipe


def consumer(p, name):
    produce, consume = p
    produce.close()
    while True:
        try:
            baozi = consume.recv()
            print('%s 收到包子:%s' % (name, baozi))
        except EOFError:
            break


def producer(seq, p):
    produce, consume = p
    consume.close()
    for i in seq:
        produce.send(i)


if __name__ == '__main__':
    produce, consume = Pipe()

    c1 = Process(target=consumer, args=((produce, consume), 'c1'))
    c1.start()

    seq = (i for i in range(10))
    producer(seq, (produce, consume))

    produce.close()
    consume.close()

    c1.join()
    print('主进程')
```

### 多消费者竞争带来的数据不安全问题

```python
from multiprocessing import Process, Pipe, Lock


def consumer(p, name, lock):
    produce, consume = p
    produce.close()
    while True:
        lock.acquire()
        baozi = consume.recv()
        lock.release()
        if baozi:
            print('%s 收到包子:%s' % (name, baozi))
        else:
            consume.close()
            break


def producer(p, n):
    produce, consume = p
    consume.close()
    for i in range(n):
        produce.send(i)
    produce.send(None)
    produce.send(None)
    produce.close()


if __name__ == '__main__':
    produce, consume = Pipe()
    lock = Lock()
    c1 = Process(target=consumer, args=((produce, consume), 'c1', lock))
    c2 = Process(target=consumer, args=((produce, consume), 'c2', lock))
    p1 = Process(target=producer, args=((produce, consume), 10))
    c1.start()
    c2.start()
    p1.start()

    produce.close()
    consume.close()

    c1.join()
    c2.join()
    p1.join()
    print('主进程')
```

## Manager(了解)

即便是使用线程,推荐做法也是将程序设计为大量独立的线程集合,通过消息队列交换数据.这样极大地减少了对使用锁定和其他同步手段的需求，还可以扩展到分布式系统中。
**但进程间应该尽量避免通信，即便需要通信，也应该选择进程安全的工具来避免加锁带来的问题**
以后我们会尝试使用数据库来解决现在进程之间的数据共享问题

```
进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的
虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此
```

- 示例

  ```python
  # Pipe 管道 双向通信 数据不安全
  # Queue 管道+锁  双向通信 数据安全
  
  # JoinableQueue ：数据安全
  # put 和 get的一个计数机制，每次get数据之后发送task_done，put端接收到计数-1，直到计数为0就能感知到
  
  # Manager是一个类,就提供了可以进行数据共享的一个机制,提供了很多数据类型.如dict,list等
  
  from multiprocessing import Manager, Process
  import time
  import random
  
  
  def get_data(dic):
      while True:
          print(dic)
          time.sleep(1)
  
  
  def set_data(dic):
      for item in range(10):
          dic['count'] = item
          time.sleep(random.randint(0, 2))
  
  
  if __name__ == '__main__':
      m = Manager()
      d = m.dict()
  
      producer = Process(target=set_data, args=(d,))
  
      consumer = Process(target=get_data, args=(d,))
  
      producer.start()
      consumer.start()
  
      producer.join()
      consumer.join()
  ```

  ```python
  from multiprocessing import Manager, Process, Lock
  
  
  def work(d, lock):
      with lock:  # 不加锁而操作共享的数据,肯定会出现数据错乱
          d['count'] -= 1
  
  
  if __name__ == '__main__':
      lock = Lock()
      with Manager() as m:
          dic = m.dict({'count': 100})
          p_l = []
          for i in range(100):
              p = Process(target=work, args=(dic, lock))
              p_l.append(p)
              p.start()
          for p in p_l:
              p.join()
          print(dic)
  ```

## 进程池

在程序实际处理问题过程中，忙时会有成千上万的任务需要被执行，闲时可能只有零星任务。那么在成千上万个任务需要被执行的时候，我们就需要去创建成千上万个进程么？首先，创建进程需要消耗时间，销毁进程也需要消耗时间。第二即便开启了成千上万的进程，操作系统也不能让他们同时执行，这样反而会影响程序的效率。因此我们**不能无限制的根据任务开启或者结束进程**

进程池的概念，定义一个池子，在里面放上固定数量的进程，有需求来了，就拿一个池中的进程来处理任务，等到处理完毕，进程并不关闭，而是将进程再放回进程池中继续等待任务。如果有很多任务需要执行，池中的进程数量不够，任务就要等待之前的进程执行任务完毕归来，拿到空闲进程才能继续执行。也就是说，池中进程的数量是固定的，那么同一时间最多有固定数量的进程在运行。这样不会增加操作系统的调度难度，还节省了开闭进程的时间，也一定程度上能够实现并发效果

### 概念

```
Pool([numprocess  [,initializer [, initargs]]]):创建进程池

numprocess:要创建的进程数，如果省略，将默认使用cpu_count()的值
initializer：是每个工作进程启动时要执行的可调用对象，默认为None
initargs：是要传给initializer的参数组
```

- 主要方法

```
p.apply(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
'''
需要强调的是：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()
'''
 
p.apply_async(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
'''
此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果
'''
    
p.close():关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成

P.jion():等待所有工作进程退出。此方法只能在 close（）或 teminate()之后调用
```

- 其他方法

```
方法apply_async()和map_async（）的返回值是AsyncResul的实例obj。实例具有以下方法

obj.get():返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发一场。如果远程操作中引发了异常，它将在调用此方法时再次被引发。

obj.ready():如果调用完成，返回True

obj.successful():如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常
obj.wait([timeout]):等待结果变为可用。

obj.terminate()：立即终止所有工作进程，同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数
```

### 进程池和多进程效率的对比

```python
from multiprocessing import Pool
from multiprocessing import Process
import time
import os


def add(i):
    return i + 1


def test_pool():
    start_time = time.time()

    process_size = os.cpu_count() + 1

    p = Pool(process_size)
    # map 方法
    # 第一个参数: 传递方法的名称
    # 第二个参数: 传入可迭代对象, 内部会依次取每个数据, 然后调用方法
    p.map(add, range(3000))

    # 关闭进程池, 防止进一步操作
    # 如果所有操作持续挂起, 它们将在工作进程终止前完成
    p.close()

    print(f'进程池消耗 {time.time() - start_time}')


def test_process():
    start_time = time.time()

    process_list = []
    for item in range(3000):
        p = Process(target=add, args=(item,))
        process_list.append(p)

    [item.start() for item in process_list]
    [item.join() for item in process_list]

    print(f'不使用进程池消耗 {time.time() - start_time}')


if __name__ == '__main__':
    test_pool()
    test_process()

# 进程池消耗 0.015331029891967773
# 不使用进程池消耗 3.336879014968872
```

### 进程池的同步调用

```python
def get_pow_2(i):
    res = i ** 2
    current_time = time.strftime('%X')
    print(f'{current_time} , res = {res}')
    time.sleep(1)
    return res


if __name__ == '__main__':
    p = Pool(os.cpu_count() + 1)

    for item in range(10):
        # 同步调用，直到本次任务执行完毕拿到res，等待任务work执行的过程中可能有阻塞也可能没有阻塞
        # 但不管该任务是否存在阻塞，同步调用都会在原地等着
        value = p.apply(get_pow_2, (item,))

# 16:37:14 , res = 0
# 16:37:15 , res = 1
# 16:37:16 , res = 4
# 16:37:17 , res = 9
# 16:37:18 , res = 16
# 16:37:19 , res = 25
# 16:37:20 , res = 36
# 16:37:21 , res = 49
# 16:37:22 , res = 64
# 16:37:23 , res = 81
```

### 进程池的异步调用

```python
def get_pow_2(i):
    res = i ** 2
    current_time = time.strftime('%X')
    print(f'{current_time} , res = {res}')
    time.sleep(1)
    return res


if __name__ == '__main__':
    p = Pool(os.cpu_count() + 1)

    result_list = []

    for item in range(10):
        # 异步运行,根据进程池中有的进程数,每次最多 os.cpu_count() + 1 个子进程在异步执行
        # 返回结果之后,将结果放入列表,归还进程,之后再执行新的任务
        # 需要注意的是,进程池中的三个进程不会同时开启或者同时结束
        # 而是执行完一个就释放一个进程,这个进程就去接收新的任务
        value = p.apply_async(get_pow_2, (item,))
        result_list.append(value)

    # 异步apply_async用法: 如果使用异步提交的任务,主进程需要使用jion,等待进程池内任务都处理完,然后可以用get收集结果
    # 否则,主进程结束,进程池可能还没来得及执行,也就跟着一起结束了
    p.close()
    p.join()

    for res in result_list:
        # 使用get来获取apply_aync的结果, 如果是apply, 则没有get方法, 因为apply是同步执行, 立刻获取结果, 也根本无需get
        print(res.get())
```


# python基础 模拟socketserver,守护进程,锁,信号量,事件,进程间通信,队列,生产者消费者模型

## 利用Process模块实现socketserver

- 配置文件 netconfig

  ```python
  ip_port = ('127.0.0.1', 8080)
  buf = 1024
  encoding = 'utf-8'
  ```

- 服务端

  ```python
  import socket
  from config import netconfig
  from multiprocessing import Process
  
  
  def handle_conn(conn):
      while 1:
          net_data_byte = conn.recv(netconfig.buf)
          if not net_data_byte:
              break
          print(f'服务端收到数据 {net_data_byte}')
          conn.send(net_data_byte.upper())
      conn.close()
  
  
  if __name__ == '__main__':
      sk = socket.socket()
      sk.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
      sk.bind(netconfig.ip_port)
      sk.listen()
  
      print('服务器已经启动')
      while 1:
          conn, address = sk.accept()
          p = Process(target=handle_conn, args=(conn,))
          p.start()
      sk.close()
  ```

- 客户端

  ```python
  import socket
  from config import netconfig
  
  sk = socket.socket()
  sk.connect(netconfig.ip_port)
  
  while 1:
      data = input('>>> ')
      if not data or data == 'q':
          break
      sk.send(bytes(data, netconfig.encoding))
      data_recv = sk.recv(netconfig.buf).decode(netconfig.encoding)
      print(f'客户端接收数据 {data_recv}')
  sk.close()
  ```

## 守护进程

- 会随着主进程的结束而结束

- 主进程创建守护进程

  1. 守护进程会在主进程代码执行结束后就终止
  2. 守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children
  3. **进程之间是互相独立的，主进程代码运行结束，守护进程随即终止**

- 没有设置守护进程的情况下, 主进程执行完毕后, 子线程仍在运行

  ```python
  from multiprocessing import Process
  import time
  
  
  def func(name):
      time.sleep(2)
      print(name)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(5):
          p = Process(target=func, args=(f'进程{index}',))
          process_list.append(p)
  
      for item in process_list:
          item.start()
  
      print('主进程执行完毕')
  
  # 主进程执行完毕
  # 进程0
  # 进程1
  # 进程3
  # 进程2
  # 进程4
  ```

- 如果为守护进程, 主进程执行完毕, 守护进程也就完毕了

  ```python
  from multiprocessing import Process
  import time
  
  
  def func(name):
      time.sleep(2)
      print(name)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(5):
          p = Process(target=func, args=(f'进程{index}',))
          # 设置为守护进程
          p.daemon = True
          process_list.append(p)
  
      for item in process_list:
          item.start()
  
      print('主进程执行完毕')
  
  # 主进程执行完毕
  ```

## 进程的其他方法

### terminate 关闭进程

```python
from multiprocessing import Process
import time


def func(name):
    time.sleep(3)
    print(name)


if __name__ == '__main__':
    process_list = []
    for index in range(5):
        p = Process(target=func, args=(f'进程{index}',))
        process_list.append(p)

    for item in process_list:
        item.start()

    for index, item in enumerate(process_list):
        if index % 2:
            # 关闭进程
            item.terminate()

    print('主进程执行完毕')

# 注意:进程的操作是通过调用操作系统来完成的, 并不是马上能关闭

# 主进程执行完毕
# 进程0
# 进程2
# 进程4
```

### is_alive 进程是否存活

```python
from multiprocessing import Process
import time


def func(name):
    time.sleep(10)
    print(name)


if __name__ == '__main__':
    process_list = []
    for index in range(5):
        p = Process(target=func, args=(f'进程{index}',), name=f'进程{index}')
        process_list.append(p)

    for item in process_list:
        item.start()

    for index, item in enumerate(process_list):
        if index % 2:
            # 关闭进程
            item.terminate()

    time.sleep(3)

    for item in process_list:
        if item.is_alive():
            print(f'{item.name} 存活')
        else:
            print(f'{item.name} 销毁')

    print('主进程执行完毕')

# 进程0 存活
# 进程1 销毁
# 进程2 存活
# 进程3 销毁
# 进程4 存活
# 主进程执行完毕
# 进程0
# 进程2
# 进程4
```

## 锁

- 程序的异步，让多个任务可以同时在几个进程中并发处理，他们之间的运行没有顺序，一旦开启也不受我们控制。尽管并发编程让我们能更加充分的利用IO资源，但是也给我们带来了新的问题
- 当多个进程使用同一份数据资源的时候，就会引发数据安全或顺序混乱问题

### 多进程抢占输出资源

```python
import os
import time
import random
from multiprocessing import Process


def work(n):
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s:%s is done' % (n, os.getpid()))


if __name__ == '__main__':
    for i in range(3):
        p = Process(target=work, args=(i,))
        p.start()
```

### 使用锁维护执行顺序

```python
# 由并发变成了串行,牺牲了运行效率,但避免了竞争
import os
import time
import random
from multiprocessing import Process, Lock


def work(lock, n):
    lock.acquire()
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s: %s is done' % (n, os.getpid()))
    lock.release()


if __name__ == '__main__':
    lock = Lock()
    for i in range(3):
        p = Process(target=work, args=(lock, i))
        p.start()

# 0: 5242 is running
# 0: 5242 is done
# 1: 5243 is running
# 1: 5243 is done
# 2: 5244 is running
# 2: 5244 is done
```

上面这种情况虽然使用加锁的形式实现了顺序的执行，但是程序又重新变成串行了，这样确实会浪费了时间，却保证了数据的安全

### 多进程实现抢票

- 没有加锁的情况(数据不安全)

  ```python
  from multiprocessing import Process
  from multiprocessing import Lock
  from config import common
  import time
  import random
  import json
  
  
  def get_ticket():
      with open(common.TICKET_FILE_PATH, 'r', encoding=common.ENCODING) as src_f:
          json_data = json.load(src_f)
      return json_data['ticket_num']
  
  
  def write_ticket(num):
      with open(common.TICKET_FILE_PATH, 'w', encoding=common.ENCODING) as dst_f:
          ticket_dict = {'ticket_num': num}
          json.dump(ticket_dict, dst_f)
  
  
  def take_ticket(name):
      ticket_num = get_ticket()
      if ticket_num <= 0:
          print(f'{name} 进行抢票, 但是没票了')
          return
  
      time.sleep(random.randint(0, 2))
  
      ticket_num -= 1
      print(f'{name} 抢到了票, 还剩余 {ticket_num}')
      write_ticket(ticket_num)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(10):
          p = Process(target=take_ticket, args=(f'{index}',))
          process_list.append(p)
  
      for item in process_list:
          time.sleep(random.random())
          item.start()
  
  # 0 抢到了票, 还剩余 4
  # 1 抢到了票, 还剩余 3
  # 3 抢到了票, 还剩余 2
  # 4 抢到了票, 还剩余 1
  # 5 抢到了票, 还剩余 0
  # 6 进行抢票, 但是没票了
  # 2 抢到了票, 还剩余 2
  # 8 抢到了票, 还剩余 1
  # 7 抢到了票, 还剩余 1
  # 9 抢到了票, 还剩余 0
  ```

- 加锁的情形(保证了数据的安全)

  ```python
  from multiprocessing import Process
  from multiprocessing import Lock
  from config import common
  import time
  import random
  import json
  
  
  def get_ticket():
      with open(common.TICKET_FILE_PATH, 'r', encoding=common.ENCODING) as src_f:
          json_data = json.load(src_f)
      return json_data['ticket_num']
  
  
  def write_ticket(num):
      with open(common.TICKET_FILE_PATH, 'w', encoding=common.ENCODING) as dst_f:
          ticket_dict = {'ticket_num': num}
          json.dump(ticket_dict, dst_f)
  
  
  def take_ticket(name, ticket_lock):
      # 申请锁
      ticket_lock.acquire()
      ticket_num = get_ticket()
      if ticket_num <= 0:
          print(f'{name} 进行抢票, 但是没票了')
      else:
          time.sleep(random.randint(0, 2))
  
          ticket_num -= 1
          print(f'{name} 抢到了票, 还剩余 {ticket_num}')
          write_ticket(ticket_num)
      # 释放锁
      ticket_lock.release()
  
  
  if __name__ == '__main__':
      ticket_lock = Lock()
      process_list = []
      for index in range(10):
          p = Process(target=take_ticket, args=(f'{index}', ticket_lock))
          process_list.append(p)
  
      for item in process_list:
          time.sleep(random.random())
          item.start()
  
  # 0 抢到了票, 还剩余 4
  # 1 抢到了票, 还剩余 3
  # 2 抢到了票, 还剩余 2
  # 3 抢到了票, 还剩余 1
  # 4 抢到了票, 还剩余 0
  # 5 进行抢票, 但是没票了
  # 6 进行抢票, 但是没票了
  # 7 进行抢票, 但是没票了
  # 8 进行抢票, 但是没票了
  # 9 进行抢票, 但是没票了
  ```

- 加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全.虽然可以用文件共享数据实现进程间通信，但问题是：
  1. 效率低（共享数据基于文件，而文件是硬盘上的数据）
  2. 需要自己加锁处理
- 因此我们最好找寻一种解决方案能够兼顾：1、效率高（多个进程共享一块内存的数据）2、帮我们处理好锁问题。这就是mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道
  - 队列和管道都是将数据存放于内存中
  - 队列又是基于(管道+锁)实现的,可以让我们从复杂的锁问题中解脱出来
  - 我们应该尽量避免使用共享数据,尽可能使用消息传递和队列,避免处理复杂的同步和锁问题,而且在进程数目增多时,往往可以获得更好的可获展性

## 信号量

- 互斥锁同时只允许一个线程更改数据，而信号量Semaphore是同时允许一定数量的线程更改数据
- 信号量同步基于内部计数器，每调用一次acquire()，计数器减1；每调用一次release()，计数器加1.当计数器为0时，acquire()调用被阻塞。这是迪科斯彻（Dijkstra）信号量概念P()和V()的Python实现。信号量同步机制适用于访问像服务器这样的有限资源。
- 信号量与进程池的概念很像，但是要区分开，信号量涉及到加锁的概念

```python
from multiprocessing import Process, Semaphore
import time, random


def go_ktv(sem, user):
    sem.acquire()

    print('%s 占到一间ktv小屋' % user)
    # 模拟每个人在ktv中待的时间不同
    time.sleep(random.randint(1, 3))

    sem.release()


if __name__ == '__main__':
    # 最多允许4个对象拿到锁
    sem = Semaphore(4)

    process_list = []
    for i in range(13):
        p = Process(target=go_ktv, args=(sem, 'user%s' % i,))
        process_list.append(p)

    for item in process_list:
        item.start()
```

## 事件

- python线程的事件用于主线程控制其他线程的执行，事件主要提供了三个方法 set、wait、clear

- 事件处理的机制：全局定义了一个“Flag”，如果“Flag”值为 False，那么当程序执行 event.wait 方法时就会阻塞，如果“Flag”值为True，那么event.wait 方法时便不再阻塞

- ```
  clear：将“Flag”设置为False
  set：将“Flag”设置为True
  ```

### 红绿灯案例

```python
from multiprocessing import Process, Event
import time, random


def car(e, n):
    while True:
        # 进程刚开启，is_set()的值是Flase，模拟信号灯为红色
        if not e.is_set():
            print('\033[31m红灯亮\033[0m，car%s等着' % n)

            # 阻塞，等待is_set()的值变成True，模拟信号灯为绿色
            e.wait()

            print('\033[32m车%s 看见绿灯亮了\033[0m' % n)
            time.sleep(random.randint(3, 6))

            # 如果is_set()的值是Flase，也就是红灯，仍然回到while语句开始
            if not e.is_set():
                continue

            print('车开远了,car', n)
            break


def police_car(e, n):
    while True:
        # 进程刚开启，is_set()的值是Flase，模拟信号灯为红色
        if not e.is_set():
            print('\033[31m红灯亮\033[0m，car%s等着' % n)

            # 阻塞,等待设置等待时间,等待0.1s之后没有等到绿灯就闯红灯走了
            e.wait(0.1)

            if not e.is_set():
                print('\033[33m红灯,警车先走\033[0m,car %s' % n)
            else:
                print('\033[33;46m绿灯,警车走\033[0m,car %s' % n)
        break


def traffic_lights(e, inverval):
    while True:
        time.sleep(inverval)
        if e.is_set():
            print('######', e.is_set())
            # 将is_set()的值设置为False
            e.clear()
        else:
            # 将is_set()的值设置为True
            e.set()
            print('***********', e.is_set())


if __name__ == '__main__':
    e = Event()
    for i in range(10):
        # 创建是个进程控制10辆车
        p = Process(target=car, args=(e, i,))
        p.start()

    for i in range(5):
        # 创建5个进程控制5辆警车
        p = Process(target=police_car, args=(e, i,))
        p.start()

    # 创建一个进程控制红绿灯
    t = Process(target=traffic_lights, args=(e, 10))
    t.start()
```

## 进程间通信

### 队列

#### 概念

- 创建共享的进程队列，Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递

  ```
  Queue([maxsize]) 
  创建共享的进程队列。
  参数 ：maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。
  底层队列使用管道和锁定实现
  ```

- 方法介绍

  ```
  Queue([maxsize]) 
  创建共享的进程队列。maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。底层队列使用管道和锁定实现。另外，还需要运行支持线程以便队列中的数据传输到底层管道中。 
  Queue的实例q具有以下方法：
  
  q.get( [ block [ ,timeout ] ] ) 
  返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。block用于控制阻塞行为，默认为True. 如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。
  
  q.get_nowait( ) 
  同q.get(False)方法。
  
  q.put(item [, block [,timeout ] ] ) 
  将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。
  
  q.qsize() 
  返回队列中目前项目的正确数量。此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。在某些系统上，此方法可能引发NotImplementedError异常。
  
  q.empty() 
  如果调用此方法时 q为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。也就是说，在返回和使用结果之间，队列中可能已经加入新的项目。
  
  q.full() 
  如果q已满，返回为True. 由于线程的存在，结果也可能是不可靠的（参考q.empty（）方法)
  
  q.close() 
  关闭队列，防止队列中加入更多数据。调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，但将在此方法完成时马上关闭。如果q被垃圾收集，将自动调用此方法。关闭队列不会在队列使用者中生成任何类型的数据结束信号或异常。例如，如果某个使用者正被阻塞在get（）操作上，关闭生产者中的队列不会导致get（）方法返回错误。
  
  q.cancel_join_thread() 
  不会再进程退出时自动连接后台线程。这可以防止join_thread()方法阻塞。
  
  q.join_thread() 
  连接队列的后台线程。此方法用于在调用q.close()方法后，等待所有队列项被消耗。默认情况下，此方法由不是q的原始创建者的所有进程调用。调用q.cancel_join_thread()方法可以禁止这种行为
  ```

#### 队列的用法

```python
"""
multiprocessing模块支持进程间通信的两种主要形式:管道和队列
都是基于消息传递实现的,但是队列接口
"""

from multiprocessing import Queue

q = Queue(3)

# put ,get ,put_nowait,get_nowait,full,empty
q.put(3)
q.put(3)
q.put(3)
# 如果队列已经满了，程序就会停在这里，等待数据被别人取走，再将数据放入队列。
# 如果队列中的数据一直不被取走，程序就会永远停在这里
# q.put(3)

try:
    # 可以使用put_nowait，如果队列满了不会阻塞，但是会因为队列满了而报错
    # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去，但是会丢掉这个消息
    q.put_nowait(3)
except Exception as ex:
    print('队列已经满了')

# 因此，我们再放入数据之前，可以先看一下队列的状态，如果已经满了，就不继续put了。
# 队列的数据是否存满
print(q.full())  # True

print(q.get())
print(q.get())
print(q.get())
# 同put方法一样，如果队列已经空了，那么继续取就会出现阻塞
# print(q.get())

try:
    # 可以使用get_nowait，如果队列满了不会阻塞，但是会因为没取到值而报错
    # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去
    q.get_nowait(3)
except Exception as ex:
    print('队列已经空了')

# 队列是否为空
print(q.empty())  # True
```

#### 利用队列实现进程间的通信

```python
from multiprocessing import Process
from multiprocessing import Queue
import time
import random


def put_data(queue):
    while 1:
        while queue.full():
            time.sleep(1)
        data = random.randint(1, 100)
        queue.put(data)
        print(f'存放数据 {data}')


def get_data(queue, index):
    while 1:
        data = queue.get()
        print(f'进程{index} 获取到数据 {data}')


if __name__ == '__main__':
    q = Queue(3)
    process_list = []
    for index in range(10):
        p = Process(target=get_data, args=(q, f'{index}'))
        process_list.append(p)

    for item in process_list:
        item.start()

    p = Process(target=put_data, args=(q,))
    p.start()
```

## 生产者消费者模型

在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度

- ##### **为什么要使用生产者和消费者模式**

  在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式

- ##### **什么是生产者消费者模式**

  生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力

### 基于队列实现生产者消费者模型

#### 第一版

```python
from multiprocessing import Process
from multiprocessing import Queue
import random
import time


def consumer(queue, index):
    while 1:
        data = queue.get()
        print(f'消费者{index} 消费 {data}')


def producer(queue):
    for index in range(10):
        time.sleep(random.randint(0, 2))
        queue.put(index)
        print(f'生产者 生产 {index}')


if __name__ == '__main__':
    queue = Queue()
    pro = Process(target=producer, args=(queue,))

    consumer_list = []
    for index in range(5):
        p = Process(target=consumer, args=(queue, index))
        consumer_list.append(p)

    for item in consumer_list:
        item.start()

    pro.start()
```

- 问题

  1. 主进程永远不会结束

     生产者p在生产完后就结束了，但是消费者c在取空了q之后，则一直处于死循环中且卡在q.get()这一步

- 解决方法

  让生产者在生产完毕后，往队列中再发一个结束信号，这样消费者在接收到结束信号后就可以break出死循环

#### 第二版

```python
from multiprocessing import Process
from multiprocessing import Queue
import random
import time


def consumer(queue, index):
    while 1:
        data = queue.get()

        if not data:
            print(f'消费者进程{index} 结束')
            break

        print(f'消费者{index} 消费 {data}')


def producer(queue, consumer_size):
    for index in range(1, 10):
        time.sleep(random.randint(0, 2))
        queue.put(index)
        print(f'生产者 生产 {index}')

    for index in range(consumer_size):
        queue.put(None)


if __name__ == '__main__':
    # 消费者的个数
    consumer_size = 5
    # 创建队列
    queue = Queue()

    pro = Process(target=producer, args=(queue, consumer_size))

    consumer_list = []
    for index in range(consumer_size):
        p = Process(target=consumer, args=(queue, index))
        consumer_list.append(p)

    for item in consumer_list:
        item.start()

    pro.start()
```

**上述虽然解决了主进程无法关闭的问题, 但是缺点在于消费者生产数据完毕后, 对于 None 的传入, 有多少个消费者就必须传入指定个 None值**

#### 第三版

- JoinableQueue

  创建可连接的共享进程队列。这就像是一个Queue对象，但队列允许项目的使用者通知生产者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的

  ```
  JoinableQueue的实例p除了与Queue对象相同的方法之外，还具有以下方法：
  
  q.task_done() 
  使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。如果调用此方法的次数大于从队列中删除的项目数量，将引发ValueError异常。
  
  q.join() 
  生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用q.task_done()方法为止。 
  下面的例子说明如何建立永远运行的进程，使用和处理队列上的项目。生产者将项目放入队列，并等待它们被处理。
  ```

- 多生产者多消费者模型

  ```python
  from multiprocessing import Process
  from multiprocessing import JoinableQueue
  import random
  import time
  
  
  def consumer(queue, consumer_name):
      while 1:
          data = queue.get()
          print(f'消费者{consumer_name} 消费 {data}')
          # 向q.join()发送一次信号,证明一个数据已经被取走了
          queue.task_done()
  
  
  def producer(queue, producer_name):
      for index in range(1, 5):
          time.sleep(random.randint(0, 2))
          queue.put(index)
          print(f'生产者{producer_name} 生产 {index}')
  
      # 生产完毕，使用此方法进行阻塞，直到队列中所有项目均被处理
      queue.join()
  
  
  if __name__ == '__main__':
      # 生产者个数
      producer_size = 3
      # 消费者的个数
      consumer_size = 5
      # 创建队列
      queue = JoinableQueue()
  
      producer_list = []
      for index in range(producer_size):
          pro = Process(target=producer, args=(queue, index))
          producer_list.append(pro)
  
      consumer_list = []
      for index in range(consumer_size):
          # 将消费者进程设置为守护进程
          p = Process(target=consumer, args=(queue, index))
          p.daemon = True
          consumer_list.append(p)
  
      for item in consumer_list:
          item.start()
  
      for item in producer_list:
          item.start()
  
      for item in producer_list:
          # 主进程等待生产者进程执行完毕
          item.join()
  ```

